{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500000 tweets loaded\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "labels = []\n",
    "\n",
    "def load_tweets(filename, label):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            tweets.append(line.rstrip())\n",
    "            labels.append(label)\n",
    "    \n",
    "load_tweets('twitter-datasets/train_neg_full.txt', 0)\n",
    "load_tweets('twitter-datasets/train_pos_full.txt', 1)\n",
    "\n",
    "# Convert to NumPy array to facilitate indexing\n",
    "tweets = np.array(tweets)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f'{len(tweets)} tweets loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build validation set\n",
    "We use 90% of tweets for training, and 10% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2250000, 250000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1) # Reproducibility!\n",
    "\n",
    "shuffled_indices = np.random.permutation(len(tweets))\n",
    "split_idx = int(0.9 * len(tweets))\n",
    "train_indices = shuffled_indices[:split_idx]\n",
    "val_indices = shuffled_indices[split_idx:]\n",
    "\n",
    "len(train_indices), len(val_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# We only keep the 5000 most frequent words, both to reduce the computational cost and reduce overfitting\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "\n",
    "# Important: we call fit_transform on the training set, and only transform on the validation set\n",
    "X_train = vectorizer.fit_transform(tweets[train_indices])\n",
    "X_val = vectorizer.transform(tweets[val_indices])\n",
    "\n",
    "Y_train = labels[train_indices]\n",
    "Y_val = labels[val_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train a logistic classifier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(C=1e5, max_iter=100)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pred = model.predict(X_train)\n",
    "Y_val_pred = model.predict(X_val)\n",
    "\n",
    "train_accuracy = (Y_train_pred == Y_train).mean()\n",
    "val_accuracy = (Y_val_pred == Y_val).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (training set): 0.80527\n",
      "Accuracy (validation set): 0.80324\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy (training set): {train_accuracy:.05f}')\n",
    "print(f'Accuracy (validation set): {val_accuracy:.05f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Top 10 negative words\n",
      "paperback -7.733715089476916\n",
      "hardcover -6.749721263599857\n",
      "sadtweet -4.022199848355659\n",
      "audio -3.8849876465208113\n",
      "misc -3.7553966613158702\n",
      "depressing -3.63732789050843\n",
      "gutted -3.5956754364460863\n",
      "wahhh -3.521614632401248\n",
      "apparel -3.217069805985382\n",
      "fml -3.1400132802859333\n",
      "\n",
      "---- Top 10 positive words\n",
      "thx 2.057920021771283\n",
      "cantsayno 2.059860424345465\n",
      "blessed 2.1638415390167096\n",
      "smiling 2.195291992774262\n",
      "worries 2.3181506563261367\n",
      "ifindthatattractive 2.4271197000353912\n",
      "harrypotterchatuplines 2.4633027181285185\n",
      "smartnokialumia 3.1312606562595673\n",
      "waystomakemehappy 3.382280627938651\n",
      "yougetmajorpointsif 4.349066000550539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_features = model.coef_[0]\n",
    "sorted_features = np.argsort(model_features)\n",
    "top_neg = sorted_features[:10]\n",
    "top_pos = sorted_features[-10:]\n",
    "\n",
    "mapping = vectorizer.get_feature_names()\n",
    "\n",
    "print('---- Top 10 negative words')\n",
    "for i in top_neg:\n",
    "    print(mapping[i], model_features[i])\n",
    "print()\n",
    "\n",
    "print('---- Top 10 positive words')\n",
    "for i in top_pos:\n",
    "    print(mapping[i], model_features[i])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
